{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2eca799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img,  img_to_array\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "  tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0fe536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SBAUser\\AppData\\Local\\Temp\\ipykernel_21108\\856612470.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpu 연결 체크\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa22f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 윈도우 케라스에서 cpu 멀티프로세싱하기\n",
    "# 멀티프로세싱은 윈도우에서 생성기를 지원하지 않음(리눅스 가능)\n",
    "# 그래서 시퀀스로 병렬 생성기 작동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fcbaee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 증강기 인스턴스 생성\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   rotation_range = 360, # 임의의 회전 각도 범위\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip = True,\n",
    "                                   fill_mode = 'nearest') # 입력 경계 밖의 픽셀을 무엇으로 채울지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b30d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23058 images belonging to 4 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DirectoryIterator at 0x1b909e9f490>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디렉토리에 대한 경로를 사용하고 증강 데이터 배치를 생성\n",
    "# bachsize만큼 랜덤으로 추출\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                  'resample', # 디렉토리\n",
    "                  target_size = (150, 150), # 정수튜플, 이미지 크기 조정 h,w 디폴트 256, 256\n",
    "                  #target_size = (420, 360),\n",
    "                  batch_size = 16,  # 데이터 배치크기, default 32 \n",
    "                  class_mode = 'sparse') # sparse는 1d 정수 레이블\n",
    "train_generator\n",
    "# DirectoryIterator생성 튜플은 모양 이 있는 이미지 배치를 포함하는 numpy 배열이고 해당 레이블의 numpy 배열입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "081003d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 데이터 크기: (16, 150, 150, 3)\n",
      "배치 레이블 크기: (16,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    # 데이터 하나만 체크해보자\n",
    "    print('배치 데이터 크기:', data_batch.shape)\n",
    "    print('배치 레이블 크기:', labels_batch.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1145a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"converea\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 75, 75, 16)        448       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 75, 75, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 25, 25, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 25, 25, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10816)             0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 10816)            43264     \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10816)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               5538304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 4)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,712,004\n",
      "Trainable params: 5,690,372\n",
      "Non-trainable params: 21,632\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(150, 150, 3), name=\"img\")\n",
    "#inputs = keras.Input(shape=(420, 360, 3), name=\"img\")\n",
    "x = layers.Conv2D(16, 3, 2, padding = 'same', activation=\"relu\")(inputs)\n",
    "x = layers.Conv2D(32, 3, 1, padding ='same', activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(64, 3, 1, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(64, 3, 2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(512, activation=\"relu\")(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(4)(x)\n",
    "outputs = layers.Softmax()(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs, name=\"converea\")\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1918feab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1442/1442 [==============================] - 63s 41ms/step - loss: 0.4012 - accuracy: 0.8430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/converea\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/converea\\assets\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_generator, batch_size=16, epochs=1)\n",
    "tf.saved_model.save(model, './model/converea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba7c2c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./model/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42061364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 4), dtype=float32, numpy=\n",
       "array([[9.4563468e-10, 5.4678764e-02, 9.4392705e-01, 1.3941592e-03],\n",
       "       [7.5542240e-04, 8.6155653e-01, 1.3489306e-01, 2.7950178e-03],\n",
       "       [9.3327235e-25, 5.3036724e-12, 3.6653112e-09, 1.0000000e+00],\n",
       "       [3.9858091e-06, 9.9566597e-01, 4.3300167e-03, 1.1515383e-09],\n",
       "       [7.6867419e-17, 5.1018153e-04, 9.9948967e-01, 1.4369765e-07],\n",
       "       [9.3374687e-01, 6.5034725e-02, 7.1542989e-04, 5.0300546e-04],\n",
       "       [4.1906734e-08, 3.4225531e-02, 9.5804036e-01, 7.7341492e-03],\n",
       "       [1.1285094e-02, 9.8820299e-01, 5.1159383e-04, 4.3546885e-07],\n",
       "       [1.5589712e-06, 9.9989927e-01, 9.9122328e-05, 9.7095400e-13],\n",
       "       [1.7349999e-10, 1.1352651e-02, 9.8792404e-01, 7.2336290e-04],\n",
       "       [2.4555298e-03, 9.9736470e-01, 1.7974577e-04, 3.0842401e-08],\n",
       "       [1.1205381e-21, 8.5583294e-14, 4.2970008e-03, 9.9570304e-01],\n",
       "       [6.8324320e-05, 9.9980921e-01, 1.2251809e-04, 9.2505265e-10],\n",
       "       [9.7893952e-08, 9.3593293e-01, 6.4066820e-02, 1.1961136e-07],\n",
       "       [5.4997301e-01, 4.2438030e-01, 1.0387902e-02, 1.5258776e-02],\n",
       "       [4.3292002e-16, 1.2979331e-10, 2.7086701e-02, 9.7291327e-01]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded = tf.saved_model.load('./model/converea')\n",
    "\n",
    "loaded(data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae95af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3911a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dfdf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, batch_size=16, epochs=2, use_multiprocessing=True,\n",
    "                         workers=6)\n",
    "tf.saved_model.save(model, './model/converea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6fb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16c8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca5480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "os.environ['TF_CONFIG'] = json.dumps({\n",
    "    'cluster': {\n",
    "        'worker': [\"localhost:12345\", \"localhost:23456\"]\n",
    "    },\n",
    "    'task': {'type': 'worker', 'index': 0}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c40cfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815cf48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f5112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bf54ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SBAUser\\AppData\\Local\\Temp\\ipykernel_18004\\349189047.py:1: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ee633",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 4\n",
    "# `tf.data.Dataset.batch`에는 전역 배치 크기를 지정해야 하기 때문에\n",
    "# 여기서 배치 크기는 워커의 수를 곱한 크기로 늘려야 합니다. \n",
    "# 전에는 64였지만, 이제 128이 됩니다.\n",
    "GLOBAL_BATCH_SIZE = 64 * NUM_WORKERS\n",
    "train_datasets = train_generator.batch(GLOBAL_BATCH_SIZE)\n",
    "with strategy.scope():\n",
    "    multi_worker_model = build_and_compile_cnn_model()\n",
    "multi_worker_model.fit(x=train_datasets, epochs=2)\n",
    "\n",
    "# model.fit(train_generator, batch_size=16, epochs=2)\n",
    "tf.saved_model.save(multi_worker_model, './model/converea1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d89e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25f5db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
